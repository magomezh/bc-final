{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "datapath = 'resources/globalsales.csv'\n",
    "\n",
    "       \n",
    "def to_dict(datapath, year = 2021, output = 'sales'):\n",
    "    \n",
    "    cats = [\"Furniture\", \"Office Supplies\", \"Technology\"]\n",
    "    subcats = [[\"Bookcases\", \"Furnishings\", \"Tables\", \"Chairs\"],[\"Appliances\", \"Binders\", \"Envelopes\", \"Fasteners\", \"Labels\", \"Paper\", \"Storage\", \"Supplies\", \"Art\"],[\"Accessories\", \"Machines\", \"Phones\", \"Copiers\"]]\n",
    "    markets = [\"Africa\", \"Asia Pacific\", \"Europe\", \"LATAM\", \"USCA\"]\n",
    "    regions = [[\"Central Africa\", \"Eastern Africa\", \"North Africa\", \"Southern Africa\", \"Western Africa\"],[\"Central Asia\", \"Eastern Asia\", \"Oceania\", \"Southeastern Asia\", \"Southern Asia\", \"Western Asia\"],[\"Eastern Europe\", \"Northern Europe\", \"Southern Europe\", \"Western Europe\"],[\"Caribbean\", \"Central America\", \"South America\"],[\"Canada\", \"Central US\", \"Eastern US\", \"Southern US\", \"Western US\"]]\n",
    "    \n",
    "    \n",
    "    data ={}\n",
    "    fcst_list =[]\n",
    "    \n",
    "    n1=0\n",
    "    alls = 'all'\n",
    "    \n",
    "    d={}\n",
    "    d[\"cat\"] = alls\n",
    "    d[\"subcat\"] = alls\n",
    "    d[\"market\"] = alls\n",
    "    d[\"region\"] = alls                \n",
    "    d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = alls, subcat = alls, market = alls, region = alls)\n",
    "    fcst_list.append(d)\n",
    "    \n",
    "    for cat in cats:\n",
    "        d={}\n",
    "        d[\"cat\"] = cat\n",
    "        d[\"subcat\"] = alls\n",
    "        d[\"market\"] = alls\n",
    "        d[\"region\"] = alls                \n",
    "        d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = cat, subcat = alls, market = alls, region = alls)\n",
    "        fcst_list.append(d)\n",
    "\n",
    "        subcat_u = subcats[n1]\n",
    "        n1 = n1 + 1\n",
    "        for subcat in subcat_u:\n",
    "            d={}\n",
    "            d[\"cat\"] = cat\n",
    "            d[\"subcat\"] = subcat\n",
    "            d[\"market\"] = alls\n",
    "            d[\"region\"] = alls\n",
    "            d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = cat, subcat = subcat, market = alls, region = alls)\n",
    "            fcst_list.append(d)\n",
    "            n2 = 0\n",
    "            for market in markets:\n",
    "                d={}\n",
    "                d[\"cat\"] = cat\n",
    "                d[\"subcat\"] = subcat\n",
    "                d[\"market\"] = market\n",
    "                d[\"region\"] = alls\n",
    "                d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = cat, subcat = subcat, market = market, region = alls)\n",
    "                fcst_list.append(d)\n",
    "                \n",
    "    data[\"fcsts\"] = fcst_list\n",
    "                \n",
    "    return data\n",
    "\n",
    "      \n",
    "def to_json_test(datapath, year = 2021, output = 'sales'):\n",
    "    \n",
    "    cats = [\"Furniture\", \"Office Supplies\", \"Technology\"]\n",
    "    subcats = [[\"Bookcases\", \"Furnishings\", \"Tables\", \"Chairs\"],[\"Appliances\", \"Binders\", \"Envelopes\", \"Fasteners\", \"Labels\", \"Paper\", \"Storage\", \"Supplies\", \"Art\"],[\"Accessories\", \"Machines\", \"Phones\", \"Copiers\"]]\n",
    "    markets = [\"Africa\", \"Asia Pacific\", \"Europe\", \"LATAM\", \"USCA\"]\n",
    "    regions = [[\"Central Africa\", \"Eastern Africa\", \"North Africa\", \"Southern Africa\", \"Western Africa\"],[\"Central Asia\", \"Eastern Asia\", \"Oceania\", \"Southeastern Asia\", \"Southern Asia\", \"Western Asia\"],[\"Eastern Europe\", \"Northern Europe\", \"Southern Europe\", \"Western Europe\"],[\"Caribbean\", \"Central America\", \"South America\"],[\"Canada\", \"Central US\", \"Eastern US\", \"Southern US\", \"Western US\"]]\n",
    "    \n",
    "    \n",
    "    data =[]\n",
    "    n1=0\n",
    "    alls = 'all'\n",
    "    \n",
    "    for cat in cats:\n",
    "        d={}\n",
    "        d[\"cat\"] = cat\n",
    "        d[\"subcat\"] = alls\n",
    "        d[\"market\"] = alls\n",
    "        d[\"region\"] = alls                \n",
    "        d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = cat, subcat = alls, market = alls, region = alls)\n",
    "        data.append(d)\n",
    "\n",
    "        subcat_u = subcats[n1]\n",
    "        n1 = n1 + 1\n",
    "        for subcat in subcat_u:\n",
    "            d={}\n",
    "            d[\"cat\"] = cat\n",
    "            d[\"subcat\"] = subcat\n",
    "            d[\"market\"] = alls\n",
    "            d[\"region\"] = alls\n",
    "            d[\"y\"] ,d[\"y_fcst\"] ,d[\"MSE\"], d[\"MAPE\"] = return_forecast(datapath, year = year, output = output, cat = cat, subcat = subcat, market = alls, region = alls)\n",
    "            data.append(d)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def filterdata(df1,output='sales', cat = \"all\", subcat = \"all\", market = \"all\", region = \"all\" ):\n",
    "    \n",
    "    if cat != \"all\":\n",
    "        df2 = df1[df1['cat']==cat]\n",
    "    else:\n",
    "        df2 = df1\n",
    "    if subcat != \"all\":\n",
    "        df3 = df2[df2['subcat']==subcat]\n",
    "    else:\n",
    "        df3 = df2\n",
    "    if market != \"all\":\n",
    "        df4 = df3[df3['market']==market]\n",
    "    else:\n",
    "        df4 = df3\n",
    "    if region != \"all\":\n",
    "        df5 = df4[df4['region']==region]\n",
    "    else:\n",
    "        df5 = df4\n",
    "                \n",
    "    df_filtered = df5.groupby(['date'], as_index = False)[\"{}\".format(output)].sum()\n",
    "    df_filtered = df_filtered.set_index('date').sort_values(by = 'date')\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def to_integer(dt_time):\n",
    "    return (dt_time.year-2016)*12 + dt_time.month\n",
    "\n",
    "def clean_data(datapath, output='sales', cat = \"all\", subcat = \"all\", market = \"all\", region = \"all\"):\n",
    "    df = pd.read_csv(datapath,encoding = \"ISO-8859-1\")\n",
    "    df.drop(df.columns[[0,1,2,3,5,6,7,8,9,10,11,12,13,16,17,25]], axis=1, inplace=True)\n",
    "    df.columns =['date',\"region\",\"market\",\"subcat\",\"cat\",\"sales\",\"quantity\",\"discount\",\"profit\",\"shippingcost\"]\n",
    "    df['date'] = df['date'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\n",
    "    df['date'] = df['date'].apply(lambda dt: dt.replace(day=1))\n",
    "    df_main = df.groupby(['date','region','market','subcat','cat'], as_index = False)['sales','profit','shippingcost'].sum()\n",
    "    result_df = filterdata(df_main, output=output, cat =cat, subcat =subcat, market =market, region =region)\n",
    "    return result_df\n",
    "\n",
    "def forecast_period(year):\n",
    "    fcst_date = []\n",
    "    for year in np.arange(2020,year+1):\n",
    "        for month in np.arange(1,13):\n",
    "            fcst_date.append(dt.datetime(year,month,1))\n",
    "    return fcst_date\n",
    "\n",
    "def return_forecast(datapath, year = 2021, output='sales', cat = \"all\", subcat = \"all\", market = \"all\", region = \"all\"):\n",
    "\n",
    "    result_df = clean_data(datapath, output=output, cat =cat, subcat =subcat, market =market, region =region)\n",
    "\n",
    "    sum_2017 = result_df[result_df.index.year == 2017] .sum()\n",
    "    sum_2018 = result_df[result_df.index.year == 2018] .sum()\n",
    "    sum_2019 = result_df[result_df.index.year == 2019] .sum()\n",
    "\n",
    "    growth_2018 =  sum_2018 / sum_2017\n",
    "    growth_2019 =  sum_2018 / sum_2017\n",
    "\n",
    "    Avg_growth = (growth_2018 + growth_2019)/2\n",
    "\n",
    "    X_forecast = forecast_period(year)\n",
    "\n",
    "    for row in X_forecast:\n",
    "        result_df.loc[row] = result_df.iloc[-12] * Avg_growth\n",
    "\n",
    "    df_diff = result_df.copy()\n",
    "\n",
    "    df_diff['prev'] = df_diff[output].shift(1)\n",
    "    df_diff['date'] = df_diff.index\n",
    "    df_diff['diff'] = (df_diff[output] - df_diff['prev'])\n",
    "\n",
    "    #create dataframe for transformation from time series to supervised\n",
    "    df_supervised = df_diff.drop(['prev'],axis=1)\n",
    "    #adding lags\n",
    "    for inc in range(1,13):\n",
    "        field_name = 'lag_' + str(inc)\n",
    "        df_supervised[field_name] = df_supervised['diff'].shift(inc)\n",
    "\n",
    "    #drop null values\n",
    "    df_supervised = df_supervised.dropna().reset_index(drop=True)\n",
    "\n",
    "    #import MinMaxScaler and create a new dataframe for LSTM model\n",
    "    df_model = df_supervised.drop([output,'date'],axis=1)\n",
    "\n",
    "    #split train and test set\n",
    "    train_set, test_set = df_model[0:-(len(X_forecast)+12)].values, df_model[-(len(X_forecast)+12):].values\n",
    "\n",
    "    #apply Min Max Scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "\n",
    "    # reshape training set\n",
    "    train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "\n",
    "    # reshape test set\n",
    "    test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "\n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mape'])\n",
    "    model.fit(X_train, y_train, nb_epoch=100, batch_size=1, verbose=0, shuffle=False)\n",
    "    accuracy = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "\n",
    "    y_pred = model.predict(X_test,batch_size=1)\n",
    "    #for multistep prediction, you need to replace X_test values with the predictions coming from t-1\n",
    "\n",
    "    #reshape y_pred\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])\n",
    "\n",
    "    #rebuild test set for inverse transform\n",
    "    pred_test_set = []\n",
    "    for index in range(0,len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index],X_test[index]],axis=1))\n",
    "\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])\n",
    "    pred_test_set_inverted = scaler.inverse_transform(pred_test_set)\n",
    "\n",
    "    #create dataframe that shows the predicted sales\n",
    "    result_list = []\n",
    "    dates = list(result_df[-(len(X_forecast)+13):].index)\n",
    "    act_value = list(result_df[-(len(X_forecast)+13):][output])\n",
    "\n",
    "    for index in range(0,len(pred_test_set_inverted)):\n",
    "        result_dict = {}\n",
    "        result_dict['pred_value'] = int(pred_test_set_inverted[index][0] + act_value[index])\n",
    "        result_dict['date'] = dates[index+1]\n",
    "        result_list.append(result_dict)\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    #for multistep prediction, replace act_sales with the predicted sales\n",
    "\n",
    "    #merge with actual sales dataframe\n",
    "    df_sales_pred = pd.merge(result_df,df_result,on='date',how='left')\n",
    "    df_sales_pred.iloc[-(len(X_forecast)):,1] = np.nan\n",
    "\n",
    "    return list(df_sales_pred[output]), list(df_sales_pred['pred_value']), accuracy[0], accuracy[1]\n",
    "\n",
    "def return_model(datapath, year = 2021, output='sales', cat = \"all\", subcat = \"all\", market = \"all\", region = \"all\"):\n",
    "\n",
    "    result_df = clean_data(datapath, output=output, cat =cat, subcat =subcat, market =market, region =region)\n",
    "\n",
    "    df_diff = result_df.copy()\n",
    "\n",
    "    df_diff['prev'] = df_diff[output].shift(1)\n",
    "    df_diff['date'] = df_diff.index\n",
    "    df_diff['diff'] = (df_diff[output] - df_diff['prev'])\n",
    "\n",
    "    #create dataframe for transformation from time series to supervised\n",
    "    df_supervised = df_diff.drop(['prev'],axis=1)\n",
    "    #adding lags\n",
    "    for inc in range(1,13):\n",
    "        field_name = 'lag_' + str(inc)\n",
    "        df_supervised[field_name] = df_supervised['diff'].shift(inc)\n",
    "\n",
    "    #drop null values\n",
    "    df_supervised = df_supervised.dropna().reset_index(drop=True)\n",
    "\n",
    "    #import MinMaxScaler and create a new dataframe for LSTM model\n",
    "    df_model = df_supervised.drop([output,'date'],axis=1)\n",
    "\n",
    "    #split train and test set\n",
    "    train_set, test_set = df_model[0:-(12)].values, df_model[-(12):].values\n",
    "\n",
    "    #apply Min Max Scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "\n",
    "    # reshape training set\n",
    "    train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "\n",
    "    # reshape test set\n",
    "    test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "\n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mape'])\n",
    "    model.fit(X_train, y_train, nb_epoch=100, batch_size=1, verbose=1, shuffle=False)\n",
    "    accuracy = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "\n",
    "    y_pred = model.predict(X_test,batch_size=1)\n",
    "    #for multistep prediction, you need to replace X_test values with the predictions coming from t-1\n",
    "\n",
    "    #reshape y_pred\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])\n",
    "\n",
    "    #rebuild test set for inverse transform\n",
    "    pred_test_set = []\n",
    "    for index in range(0,len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index],X_test[index]],axis=1))\n",
    "\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])\n",
    "    pred_test_set_inverted = scaler.inverse_transform(pred_test_set)\n",
    "\n",
    "    #create dataframe that shows the predicted sales\n",
    "    result_list = []\n",
    "    dates = list(result_df[-(13):].index)\n",
    "    act_value = list(result_df[-(13):][output])\n",
    "\n",
    "    for index in range(0,len(pred_test_set_inverted)):\n",
    "        result_dict = {}\n",
    "        result_dict['pred_value'] = int(pred_test_set_inverted[index][0] + act_value[index])\n",
    "        result_dict['date'] = dates[index+1]\n",
    "        result_list.append(result_dict)\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    #for multistep prediction, replace act_sales with the predicted sales\n",
    "\n",
    "    #merge with actual sales dataframe\n",
    "    df_sales_pred = pd.merge(result_df,df_result,on='date',how='left')\n",
    "\n",
    "    return list(df_sales_pred[output]), list(df_sales_pred['pred_value']), accuracy[0], accuracy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 5ms/step\n",
      "36/36 [==============================] - 0s 5ms/step\n",
      "36/36 [==============================] - 0s 5ms/step\n",
      "36/36 [==============================] - 0s 6ms/step\n",
      "36/36 [==============================] - 0s 6ms/step\n",
      "36/36 [==============================] - 0s 7ms/step\n",
      "36/36 [==============================] - 0s 7ms/step\n",
      "36/36 [==============================] - 0s 8ms/step\n",
      "36/36 [==============================] - 0s 8ms/step\n",
      "36/36 [==============================] - 0s 9ms/step\n",
      "36/36 [==============================] - 0s 9ms/step\n",
      "36/36 [==============================] - 0s 9ms/step\n",
      "36/36 [==============================] - 0s 9ms/step\n",
      "36/36 [==============================] - 0s 10ms/step\n",
      "36/36 [==============================] - 0s 10ms/step\n",
      "36/36 [==============================] - 0s 10ms/step\n",
      "36/36 [==============================] - 0s 11ms/step\n",
      "36/36 [==============================] - 0s 11ms/step\n",
      "36/36 [==============================] - 0s 12ms/step\n",
      "36/36 [==============================] - 0s 13ms/step\n",
      "36/36 [==============================] - 0s 13ms/step\n",
      "36/36 [==============================] - 0s 13ms/step\n",
      "36/36 [==============================] - 0s 14ms/step\n",
      "36/36 [==============================] - 1s 14ms/step\n",
      "36/36 [==============================] - 1s 14ms/step\n",
      "36/36 [==============================] - 1s 15ms/step\n",
      "36/36 [==============================] - 1s 15ms/step\n",
      "36/36 [==============================] - 1s 16ms/step\n",
      "36/36 [==============================] - 1s 16ms/step\n",
      "36/36 [==============================] - 1s 16ms/step\n",
      "36/36 [==============================] - 1s 17ms/step\n",
      "36/36 [==============================] - 1s 17ms/step\n",
      "36/36 [==============================] - 1s 18ms/step\n",
      "36/36 [==============================] - 1s 18ms/step\n",
      "36/36 [==============================] - 1s 19ms/step\n",
      "36/36 [==============================] - 1s 20ms/step\n",
      "36/36 [==============================] - 1s 20ms/step\n",
      "36/36 [==============================] - 1s 20ms/step\n",
      "36/36 [==============================] - 1s 20ms/step\n",
      "36/36 [==============================] - 1s 21ms/step\n",
      "36/36 [==============================] - 1s 22ms/step\n",
      "36/36 [==============================] - 1s 23ms/step\n",
      "36/36 [==============================] - 1s 24ms/step\n",
      "36/36 [==============================] - 1s 24ms/step\n",
      "36/36 [==============================] - 1s 25ms/step\n",
      "36/36 [==============================] - 1s 26ms/step\n",
      "36/36 [==============================] - 1s 26ms/step\n",
      "36/36 [==============================] - 1s 25ms/step\n",
      "36/36 [==============================] - 1s 27ms/step\n",
      "36/36 [==============================] - 1s 28ms/step\n",
      "36/36 [==============================] - 1s 29ms/step\n",
      "36/36 [==============================] - 1s 35ms/step\n",
      "36/36 [==============================] - 1s 30ms/step\n",
      "36/36 [==============================] - 1s 31ms/step\n",
      "36/36 [==============================] - 1s 31ms/step\n",
      "36/36 [==============================] - 1s 31ms/step\n",
      "36/36 [==============================] - 1s 33ms/step\n",
      "36/36 [==============================] - 1s 33ms/step\n",
      "36/36 [==============================] - 1s 34ms/step\n",
      "36/36 [==============================] - 1s 35ms/step\n",
      "36/36 [==============================] - 1s 35ms/step\n",
      "36/36 [==============================] - 1s 35ms/step\n",
      "36/36 [==============================] - 1s 36ms/step\n",
      "36/36 [==============================] - 2s 42ms/step\n",
      "36/36 [==============================] - 1s 37ms/step\n",
      "36/36 [==============================] - 1s 37ms/step\n",
      "36/36 [==============================] - 1s 40ms/step\n",
      "36/36 [==============================] - 2s 42ms/step\n",
      "36/36 [==============================] - 1s 41ms/step\n",
      "36/36 [==============================] - 2s 42ms/step\n",
      "36/36 [==============================] - 2s 43ms/step\n",
      "36/36 [==============================] - 2s 44ms/step\n",
      "36/36 [==============================] - 2s 46ms/step\n",
      "36/36 [==============================] - 2s 47ms/step\n",
      "36/36 [==============================] - 2s 49ms/step\n",
      "36/36 [==============================] - 2s 46ms/step\n",
      "36/36 [==============================] - 2s 50ms/step\n",
      "36/36 [==============================] - 2s 49ms/step\n",
      "36/36 [==============================] - 2s 50ms/step\n",
      "36/36 [==============================] - 2s 51ms/step\n",
      "36/36 [==============================] - 2s 53ms/step\n",
      "36/36 [==============================] - 2s 51ms/step\n",
      "36/36 [==============================] - 2s 53ms/step\n",
      "36/36 [==============================] - 2s 53ms/step\n",
      "36/36 [==============================] - 2s 51ms/step\n",
      "36/36 [==============================] - 2s 52ms/step\n",
      "36/36 [==============================] - 2s 52ms/step\n",
      "36/36 [==============================] - 2s 52ms/step\n",
      "36/36 [==============================] - 2s 51ms/step\n",
      "36/36 [==============================] - 2s 55ms/step\n",
      "36/36 [==============================] - 2s 55ms/step\n",
      "36/36 [==============================] - 2s 55ms/step\n",
      "36/36 [==============================] - 2s 56ms/step\n",
      "36/36 [==============================] - 2s 56ms/step\n",
      "36/36 [==============================] - 2s 56ms/step\n",
      "36/36 [==============================] - 2s 60ms/step\n",
      "36/36 [==============================] - 2s 60ms/step\n",
      "36/36 [==============================] - 2s 61ms/step\n",
      "36/36 [==============================] - 2s 61ms/step\n"
     ]
    }
   ],
   "source": [
    "dict_fcsts = to_dict(datapath)\n",
    "data = dict_fcsts['fcsts']\n",
    "df_final = pd.DataFrame.from_dict(data)\n",
    "df_final.to_csv('fcstresults.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('forecastresults2.json', 'w') as fout:\n",
    "    json.dump(json_forecasts, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>subcat</th>\n",
       "      <th>market</th>\n",
       "      <th>region</th>\n",
       "      <th>y</th>\n",
       "      <th>y_fcst</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>[34463.740000000005, 38291.34, 37963.060000000...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.540071</td>\n",
       "      <td>63.653809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>[13401.999999999998, 15019.529999999999, 8975....</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.437355</td>\n",
       "      <td>127.355141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Africa</td>\n",
       "      <td>all</td>\n",
       "      <td>[3739.95, 553.3100000000001, 1288.949999999999...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.584718</td>\n",
       "      <td>352.884155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Asia Pacific</td>\n",
       "      <td>all</td>\n",
       "      <td>[156.96, 6882.69, 2066.1, 1669.43, 6787.87, 75...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.202728</td>\n",
       "      <td>713.157349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Europe</td>\n",
       "      <td>all</td>\n",
       "      <td>[4508.610000000001, 7160.73, 4227.96, 1006.53,...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.511192</td>\n",
       "      <td>113.573914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Africa</td>\n",
       "      <td>all</td>\n",
       "      <td>[1515.08, 444.96, 551.37, 359.53, 974.43, 175....</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>1.021608</td>\n",
       "      <td>165.211029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Asia Pacific</td>\n",
       "      <td>all</td>\n",
       "      <td>[2786.43, 4685.35, 5126.31, 2343.2599999999998...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.783446</td>\n",
       "      <td>302.168213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Europe</td>\n",
       "      <td>all</td>\n",
       "      <td>[2276.81, 3704.09, 791.3100000000001, 785.34, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.776641</td>\n",
       "      <td>234.362610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>LATAM</td>\n",
       "      <td>all</td>\n",
       "      <td>[2387.3199999999997, 2719.0200000000004, 3876....</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.556263</td>\n",
       "      <td>406.201538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>USCA</td>\n",
       "      <td>all</td>\n",
       "      <td>[630.1, 2519.95, 850.69, 1749.97, 1799.97, 252...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>2.659350</td>\n",
       "      <td>171.894089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cat     subcat        market region  \\\n",
       "0     Furniture        all           all    all   \n",
       "1     Furniture  Bookcases           all    all   \n",
       "2     Furniture  Bookcases        Africa    all   \n",
       "3     Furniture  Bookcases  Asia Pacific    all   \n",
       "4     Furniture  Bookcases        Europe    all   \n",
       "..          ...        ...           ...    ...   \n",
       "100  Technology    Copiers        Africa    all   \n",
       "101  Technology    Copiers  Asia Pacific    all   \n",
       "102  Technology    Copiers        Europe    all   \n",
       "103  Technology    Copiers         LATAM    all   \n",
       "104  Technology    Copiers          USCA    all   \n",
       "\n",
       "                                                     y  \\\n",
       "0    [34463.740000000005, 38291.34, 37963.060000000...   \n",
       "1    [13401.999999999998, 15019.529999999999, 8975....   \n",
       "2    [3739.95, 553.3100000000001, 1288.949999999999...   \n",
       "3    [156.96, 6882.69, 2066.1, 1669.43, 6787.87, 75...   \n",
       "4    [4508.610000000001, 7160.73, 4227.96, 1006.53,...   \n",
       "..                                                 ...   \n",
       "100  [1515.08, 444.96, 551.37, 359.53, 974.43, 175....   \n",
       "101  [2786.43, 4685.35, 5126.31, 2343.2599999999998...   \n",
       "102  [2276.81, 3704.09, 791.3100000000001, 785.34, ...   \n",
       "103  [2387.3199999999997, 2719.0200000000004, 3876....   \n",
       "104  [630.1, 2519.95, 850.69, 1749.97, 1799.97, 252...   \n",
       "\n",
       "                                                y_fcst       MSE        MAPE  \n",
       "0    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.540071   63.653809  \n",
       "1    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.437355  127.355141  \n",
       "2    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.584718  352.884155  \n",
       "3    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.202728  713.157349  \n",
       "4    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.511192  113.573914  \n",
       "..                                                 ...       ...         ...  \n",
       "100  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  1.021608  165.211029  \n",
       "101  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.783446  302.168213  \n",
       "102  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.776641  234.362610  \n",
       "103  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  0.556263  406.201538  \n",
       "104  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  2.659350  171.894089  \n",
       "\n",
       "[105 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
